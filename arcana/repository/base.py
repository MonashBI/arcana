from builtins import object
from abc import ABCMeta, abstractmethod
from nipype.interfaces.base import (
    traits, DynamicTraitedSpec, Undefined, File, Directory,
    BaseInterface)
from arcana.node import Node
from arcana.dataset import (
    BaseDatum, DatasetSpec, FieldSpec, BaseField, BaseDataset,
    DatasetMatch, FieldMatch)
from arcana.exception import ArcanaError
from arcana.utils import PATH_SUFFIX, FIELD_SUFFIX
from future.utils import with_metaclass

PATH_TRAIT = traits.Either(File(exists=True), Directory(exists=True))
FIELD_TRAIT = traits.Either(traits.Int, traits.Float, traits.Str)
MULTIPLICITIES = ('per_session', 'per_subject', 'per_visit', 'per_project')


class Repository(with_metaclass(ABCMeta, object)):
    """
    Abstract base class for all Repository systems, DaRIS, XNAT and local file
    system. Sets out the interface that all Repository classes should implement.
    """

    def __init__(self):
        self._connection_depth = 0

    def __enter__(self):
        # This allows the repository to be used within nested contexts
        # but still only use one connection. This is useful for calling
        # methods that need connections, and therefore control their
        # own connection, in batches
        if self._connection_depth == 0:
            self.connect()
        self._connection_depth += 1
        return self

    def __exit__(self, exception_type, exception_value, traceback):  # @UnusedVariable @IgnorePep8
        self._connection_depth -= 1
        if self._connection_depth == 0:
            self.disconnect()

    def connect(self):
        """
        If a connection session is required to the repository,
        manage it here
        """

    def disconnect(self):
        """
        If a connection session is required to the repository,
        manage it here
        """

    def cache(self, datum):
        """
        If the repository is remote, cache the dataset or field here
        """
        pass

    @abstractmethod
    def insert(self, datum):
        """
        Insert the given dataset or field into the repository
        """

    @abstractmethod
    def tree(self):
        """
        Should return a Project object representing the current state
        of the repository
        """

    def source(self, inputs, name=None):
        """
        Returns a NiPype node that gets the input data from the repository
        system. The input spec of the node's interface should inherit from
        RepositorySourceInputSpec

        Parameters
        ----------
        project_id : str
            The ID of the project to return the sessions for
        inputs : list(Dataset|Field)
            An iterable of arcana.Dataset or arcana.Field
            objects, which specify the datasets to extract from the
            repository system
        name : str
            Name of the NiPype node
        study_name: str
            Prefix used to distinguish datasets generated by a particular
            study. Used for derived datasets only
        """
        if name is None:
            name = "{}_source".format(self.type)
        inputs = list(inputs)  # protected against iterators
        datasets = []
        fields = []
        for inpt in inputs:
            if isinstance(inpt, DatasetMatch):
                datasets.append(inpt.matches)
            elif isinstance(inpt, DatasetSpec):
                datasets.append(inpt)
            if isinstance(inpt, FieldMatch):
                fields.append(inpt.matches)
            elif isinstance(inpt, FieldSpec):
                fields.append(inpt)
        return Node(RepositorySource(datasets, fields), name=name)

    def sink(self, outputs, frequency='per_session', name=None):
        """
        Returns a NiPype node that puts the output data back to the repository
        system. The input spec of the node's interface should inherit from
        RepositorySinkInputSpec

        Parameters
        ----------
        project_id : str
            The ID of the project to return the sessions for
        outputs : List(BaseFile|Field) | list(
            An iterable of arcana.Dataset arcana.Field objects,
            which specify the datasets to put into the repository system
        name : str
            Name of the NiPype node
        study_name: str
            Prefix used to distinguish datasets generated by a particular
            study. Used for derived datasets only

        """
        if name is None:
            name = "{}_{}_sink".format(self.type, frequency)
        outputs = list(outputs)  # protected against iterators
        if frequency.startswith('per_session'):
            sink_class = RepositorySessionSink
        elif frequency.startswith('per_subject'):
            sink_class = RepositorySubjectSink
        elif frequency.startswith('per_visit'):
            sink_class = RepositoryVisitSink
        elif frequency.startswith('per_project'):
            sink_class = RepositoryProjectSink
        else:
            raise ArcanaError(
                "Unrecognised frequency '{}' can be one of '{}'"
                .format(frequency,
                        "', '".join(BaseDatum.MULTIPLICITY_OPTIONS)))
        datasets = [o for o in outputs if isinstance(o, BaseDataset)]
        fields = [o for o in outputs if isinstance(o, BaseField)]
        return Node(sink_class(datasets, fields), name=name)

    def __ne__(self, other):
        return not (self == other)


class BaseRepositoryNode(BaseInterface):
    """
    Parameters
    ----------
    infields : list of str
        Indicates the input fields to be dynamically created

    outfields: list of str
        Indicates output fields to be dynamically created

    See class examples for usage

    """

    def __init__(self, datasets, fields):
        super(BaseRepositoryNode, self).__init__()
        self._datasets = datasets
        self._fields = fields

    def __eq__(self, other):
        try:
            return (self.datasets == other.datasets and
                    self.fields == other.fields)
        except AttributeError:
            return False

    def __repr__(self):
        return "{}(datasets={}, fields={})".format(
            type(self).__name__, self.datasets, self.fields)

    def __ne__(self, other):
        return not self == other

    def _run_interface(self, runtime, *args, **kwargs):  # @UnusedVariable
        return runtime

    @property
    def datasets(self):
        return self._datasets

    @property
    def fields(self):
        return self._fields

    @classmethod
    def _add_trait(cls, spec, name, trait_type):
        spec.add_trait(name, trait_type)
        spec.trait_set(trait_change_notify=False, **{name: Undefined})
        # Access the trait (not sure why but this is done in add_traits
        # so I have also done it here
        getattr(spec, name)


class RepositorySourceSpec(DynamicTraitedSpec):
    """
    Base class for repository sink and source input specifications.
    """
    subject_id = traits.Str(mandatory=True, desc="The subject ID")
    visit_id = traits.Str(mandatory=True, usedefult=True,
                          desc="The visit ID")


class RepositorySource(BaseRepositoryNode):
    """
    Parameters
    ----------
    datasets: list
        List of all datasets to be extracted from the repository
    fields: list
        List of all the fields that are to be extracted from the repository
    study_name: str
        Prefix prepended onto derived dataset "names"
    """

    input_spec = RepositorySourceSpec
    output_spec = RepositorySourceSpec
    _always_run = True

    def _outputs(self):
        outputs = super(RepositorySource, self)._outputs()
        # Add output datasets
        for dataset in self.datasets:
            self._add_trait(outputs, dataset.name + PATH_SUFFIX,
                            PATH_TRAIT)
        # Add output fields
        for field in self.fields:
            self._add_trait(outputs, field.name + FIELD_SUFFIX,
                            field.dtype)
        return outputs

    def _list_outputs(self):
        # Directory that holds session-specific
        outputs = {}
        # Source datasets
        for dataset in self.datasets:
            outputs[dataset.name + PATH_SUFFIX] = dataset.path(
                subject_id=self.inputs.subject_id,
                visit_id=self.inputs.visit_id)
        # Source fields from JSON file
        for freq, spec_grp in groupby(
            sorted(self.fields, key=attrgetter('frequency')),
                key=attrgetter('frequency')):
            # Load fields JSON, locking to prevent read/write conflicts
            # Would be better if only checked if locked to allow
            # concurrent reads but not possible with multi-process
            # locks (in my understanding at least).
            fpath = self.fields_path(freq)
            try:
                with InterProcessLock(
                    fpath + LOCK,
                        logger=logger), open(fpath, 'r') as f:
                    fields = json.load(f)
            except IOError as e:
                if e.errno == errno.ENOENT:
                    fields = {}
                else:
                    raise
            for field in spec_grp:
                outputs[field.name + FIELD_SUFFIX] = field.dtype(
                    fields[self.prefix_study_name(field.name,
                                                  field.is_spec)])
        return outputs


class BaseRepositorySinkSpec(DynamicTraitedSpec):
    pass


class RepositorySinkInputSpec(BaseRepositorySinkSpec):

    subject_id = traits.Str(mandatory=True, desc="The subject ID"),
    visit_id = traits.Str(mandatory=False,
                            desc="The session or derived group ID")


class RepositorySubjectSinkInputSpec(BaseRepositorySinkSpec):

    subject_id = traits.Str(mandatory=True, desc="The subject ID")


class RepositoryVisitSinkInputSpec(BaseRepositorySinkSpec):

    visit_id = traits.Str(mandatory=True, desc="The visit ID")


class RepositoryProjectSinkInputSpec(BaseRepositorySinkSpec):
    pass


class BaseRepositorySinkOutputSpec(DynamicTraitedSpec):

    out_files = traits.List(PATH_TRAIT, desc='Output datasets')

    out_fields = traits.List(
        traits.Tuple(traits.Str, FIELD_TRAIT), desc='Output fields')


class RepositorySinkOutputSpec(BaseRepositorySinkOutputSpec):

    subject_id = traits.Str(desc="The subject ID")
    visit_id = traits.Str(desc="The visit ID")


class RepositorySubjectSinkOutputSpec(BaseRepositorySinkOutputSpec):

    subject_id = traits.Str(desc="The subject ID")


class RepositoryVisitSinkOutputSpec(BaseRepositorySinkOutputSpec):

    visit_id = traits.Str(desc="The visit ID")


class RepositoryProjectSinkOutputSpec(BaseRepositorySinkOutputSpec):

    project_id = traits.Str(desc="The project ID")


class BaseRepositorySink(BaseRepositoryNode):

    def __init__(self, study_name, datasets, fields):
        super(BaseRepositorySink, self).__init__(study_name, datasets,
                                                 fields)
        # Add input datasets
        for dataset in datasets:
            assert isinstance(dataset, DatasetSpec)
            self._add_trait(self.inputs, dataset.name + PATH_SUFFIX,
                            PATH_TRAIT)
        # Add input fields
        for field in fields:
            assert isinstance(field, FieldSpec)
            self._add_trait(self.inputs, field.name + FIELD_SUFFIX,
                            field.dtype)


class RepositorySessionSink(BaseRepositorySink):

    input_spec = RepositorySinkInputSpec
    output_spec = RepositorySinkOutputSpec

    frequency = 'per_session'

    def _base_outputs(self):
        outputs = self.output_spec().get()
        outputs['subject_id'] = self.inputs.subject_id
        outputs['visit_id'] = self.inputs.visit_id
        return outputs


class RepositorySubjectSink(BaseRepositorySink):

    input_spec = RepositorySubjectSinkInputSpec
    output_spec = RepositorySubjectSinkOutputSpec

    frequency = 'per_subject'

    def _base_outputs(self):
        outputs = self.output_spec().get()
        outputs['subject_id'] = self.inputs.subject_id
        return outputs


class RepositoryVisitSink(BaseRepositorySink):

    input_spec = RepositoryVisitSinkInputSpec
    output_spec = RepositoryVisitSinkOutputSpec

    frequency = 'per_visit'

    def _base_outputs(self):
        outputs = self.output_spec().get()
        outputs['visit_id'] = self.inputs.visit_id
        return outputs


class RepositoryProjectSink(BaseRepositorySink):

    input_spec = RepositoryProjectSinkInputSpec
    output_spec = RepositoryProjectSinkOutputSpec

    frequency = 'per_project'

    def _base_outputs(self):
        outputs = self.output_spec().get()
        return outputs
