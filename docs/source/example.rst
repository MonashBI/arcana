Examples
========

A basic toy example

.. code-block:: python

    from arcana import (
        Study, StudyMetaClass, InputFilesetSpec, FilesetSpec,
        InputFieldSpec, FieldSpec, ParamSpec, SwitchSpec,
        InputFileset, InputField)
    from your_package import (
        Interface1, Interface2, Interface3, methods_paper_cite, software_req,
        software_req2, matlab_req, toolbox_req)
    
    
    class ExampleStudy(Study, metaclass=StudyMetaClass):
    
        # Specify all data required or generated by instances of the Study
        # class, including their format (for filesets) or data type (for fields)
        # and generating pipeline (for derived filesets/fields)
        add_data_specs = [
            # Acquired file sets
            InputFilesetSpec('acquired_file1', text_format),
            InputFilesetSpec('acquired_file2', STD_IMAGE_FORMATS),
            # Acquired fields
            InputFieldSpec('acquired_field1', int, array=True,
                              frequency='per_subject'),
            InputFieldSpec('acquired_field2', float, optional=True),
            # "Acquired" file set with default value. Useful for
            # standard templates
            InputFilesetSpec('template1', STD_IMAGE_FORMATS,
                                frequency='per_study',
                                default=template_collectn),
            # Derived file sets
            FilesetSpec('derived_file1', text_format, 'pipeline1'),
            FilesetSpec('derived_file2', nifti_gz_format, 'pipeline1'),
            FilesetSpec('derived_file3', text_matrix_format, 'pipeline2'),
            FilesetSpec('derived_file4', dicom_format, 'pipeline3'),
            FilesetSpec('derived_file5', nifti_gz_format, 'pipeline3',
                        frequency='per_subject'),
            FilesetSpec('derived_file6', analyze_format, 'pipeline2',
                        frequency='per_visit'),
            # Derived fields
            FieldSpec('derived_field1', float, 'pipeline2'),
            FieldSpec('derived_field2', int, 'pipeline4',
                      frequency='per_study')]
    
        # Specify free parameters of the analysis, including quantitative
        # (Parameter) and qualitative (Switches) options.
        add_param_specs = [
            # Standard parameters
            ParamSpec('parameter1', 10),
            ParamSpec('parameter2', 25.8),
            # "Switch" parameters that specify a qualitative change
            # in the analysis
            SwitchSpec('node1_option', False),  # Boolean switch
            SwitchSpec('pipeline2_tool', 'toolA', ('toolA', 'toolB'))]
    
        #### Other pipeline constructors go here ####
    
        # Define a method that constructs a pipeline that produces
        # 'derived_file3'.
        def pipeline2(self, **name_maps):
    
            # Initialise a new pipeline
            pipeline = self.new_pipeline(
                name='pipeline2',
                name_maps=name_maps,
                desc="Description of the pipeline",
                citations=[methods_paper_cite])
    
            # Add a pipeline that runs "Interface1", pulling 
            # acquired file and fields 1 & derived file 2, from the repository
            # and pushing 'derived_field1' back to the repository
            node1 = pipeline.add(
                'node1',
                Interface1(
                    param1=3.5,
                    param2=self.parameter('parameter1')),
                inputs={
                    'in_file1': ('acquired_file1', text_format),
                    'in_file2': ('derived_file2', analyze_format),
                    'in_field': ('acquired_field1', int)},
                outputs={
                    'derived_field1': ('out_field', int)},
                # Conservative estimate of the wall time required
                # to execute the node
                wall_time=25,
                # The software required to be installed in the environment
                # the workflow is run in 
                requirements=[software_req1])
            
            # If 'node1_option' is set to True when the study is initialised
            # then this option is set here using tradition Nipype syntax
            if self.branch('node1_option'):
                node1.inputs.an_option = 'set-extra-option'
    
            # Depending on the tool selected in the 'pipeline2_too' switch
            # parameter, a node is added with either an Interface2 or
            # Interface3 interface 
            if self.branch('pipeline2_tool', 'toolA'):
                pipeline.add(
                    'node2',
                    Interface2(
                        param1=self.parameter('parameter2')),
                    inputs={
                        'template': ('template1', nifti_gz_format),
                        'in_file': (node1, 'out_file')},
                    outputs={
                        'derived_file3': ('out_file',
                                          text_matrix_format),
                        'derived_file6': ('out', nifti_format)},
                    wall_time=10, requirements=[software_req2])
    
            elif self.branch('pipeline2_tool', 'toolB'):
                pipeline.add(
                    'node2',
                    Interface3(),
                    inputs={
                        'template': ('template1', nifti_gz_format),
                        'in_file': (node1, 'out_file')},
                    outputs={
                        'derived_file3': ('out_file',
                                          text_matrix_format)},
                    wall_time=30, requirements=[matlab_req,
                                                toolbox1_req])
            else:
                self.unhandled_branch('pipeline2_tool')
    
            return pipeline
            
which can then be instantiated and used to generate 'derived2' with 

.. code-block:: python

    # Initialise study, selecting data corresponding to the data
    # specified in the local directory repository and parameters
    # used in the processing
    your_study = ExampleStudy(
        name='your_study',
        repository=BasicRepo('/path/to/local/archive'),
        processor=SingleProc('/my/work/dir'),
        environment=StaticEnv(),
        inputs=[
            InputFileset('acquired_file1', 'your-name-for-file1'),
            InputFileset('acquired_file2', 'your-name-for-file2'),
            InputField('acquired_field1', 'your-name-for-field1')],
        parameters={'parameter2': 50.0,
                    'node1_option': True})

    # Execute the pipelines required to generate file 5 and field 2
    # and return handle to generated data 
    file5, field2 = study.data(['derived_file5', 'derived_field2'])
    print("Generated derived file 5 at '{}'.format(file5.path))
    print("Value of generated field 2 = {}'.format(field2))
